<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Paper Tracker Daily Reading</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:wght@400;700&family=Crimson+Pro:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/style.css">
</head>
<body class="theme-light">
  <a class="skip-link" href="#main-content">跳转到正文</a>

  <header class="page-header">
    <div class="header-main">
      <p class="header-topline">
        POWER BY
        <a class="powered-by-link" href="https://github.com/RainerSeventeen/paper-tracker" target="_blank" rel="noopener noreferrer">
          RainerSeventeen/paper-tracker
        </a>
      </p>
      <h1>文献阅读简报</h1>
      <p class="header-meta">生成时间：2026-02-17 03:00:42</p>
      <div class="report-stats">
        <p><span>4</span> 个查询主题</p>
        <p><span>20</span> 篇候选论文</p>
      </div>
    </div>
    <div class="header-actions">
      <button id="expand-all" type="button" aria-label="展开所有论文详情">展开全部</button>
      <button id="collapse-all" type="button" aria-label="折叠所有论文详情">折叠全部</button>
      <button id="theme-toggle" type="button" aria-label="切换暗色模式">切换暗色模式</button>
    </div>
  </header>

  <div class="layout">
    <aside id="sidebar" class="sidebar" aria-label="查询导航">
      <p class="sidebar-title">文献目录</p>
      <p class="sidebar-note">按查询主题快速定位</p>
    </aside>
    <main id="main-content" class="content">
      <section id="query-query" class="query-section" data-query-label="视频及图像压缩" data-paper-count="5">
  <header class="query-header">
    <h2>视频及图像压缩</h2>
    <p class="query-count">5 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">A linesearch-type normal map-based semismooth Newton method for nonsmooth nonconvex composite optimization</h3>
    <p class="paper-authors">Hanfeng Zeng, Wenqing Ouyang, Andre Milzarek</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-13</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>提出一种基于线搜索的改进型信赖域半光滑牛顿法，用于求解非光滑非凸复合优化问题。该方法通过自适应参数估计避免计算Lipschitz常数，简化了算法并放宽了收敛假设，在稀疏逻辑回归、图像压缩等问题上验证了有效性。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.13000v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.13000v1" target="_blank" rel="noopener noreferrer">Abstract</a>
    <a class="paper-link" href="https://doi.org/10.1007/s10589-025-00754-0" target="_blank" rel="noopener noreferrer">DOI</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>We propose a novel linesearch variant of the trust region normal map-based semismooth Newton method developed in [Ouyang and Milzarek, Math. Program. 212(1-2), 389--435 (2025)] for solving a class of nonsmooth, nonconvex composite-type optimization problems. Our approach uses adaptive parameter estimation techniques, which allow us to avoid explicit and potentially expensive Lipschitz constant computations. We provide extensive convergence results including global convergence, convergence of the iterates under the Kurdyka-Łojasiewicz inequality, and transition to fast local q-superlinear convergence. Compared to the original trust region framework, the linesearch-based algorithm is simpler and the overall convergence analysis can be conducted under weaker assumptions -- in particular, without requiring explicit boundedness conditions on the Hessian approximations and iterates. Numerical experiments on sparse logistic regression, image compression, and nonlinear least squares with group penalty terms demonstrate the efficiency of the proposed approach.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有信赖域半光滑牛顿法需要显式计算Lipschitz常数，且依赖较强的有界性假设（如Hessian近似和迭代点的有界性）。本文旨在设计更简单、假设更弱的算法框架，避免昂贵常数计算，同时保持收敛性。</p></div>
    <div class="detail-block"><h4>Method</h4><p>在线搜索框架下改造信赖域半光滑牛顿法，引入自适应参数估计技术替代显式Lipschitz常数计算。算法结合了正常映射（normal map）和半光滑牛顿迭代，在线搜索中调整步长以保证下降性。</p></div>
    <div class="detail-block"><h4>Result</h4><p>证明了算法的全局收敛性、在Kurdyka-Łojasiewicz不等式下的迭代点收敛性，以及局部q-超线性收敛速度。数值实验在稀疏逻辑回归、图像压缩和带群惩罚的非线性最小二乘问题上展示了优于原信赖域方法的效率。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>所提线搜索算法在简化实现、放宽理论假设的同时，保持了强收敛性质，并通过自适应技术提升了计算可行性，为非光滑非凸复合优化问题提供了更实用的求解工具。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">SplitCom: Communication-efficient Split Federated Fine-tuning of LLMs via Temporal Compression</h3>
    <p class="paper-authors">Tao Li, Yulin Tang, Yiyang Song, Cong Wu, Xihui Liu, Pan Li, Xianhao Chen</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-12</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出SplitCom，一种用于大语言模型的高效通信分割联邦学习框架，通过利用训练过程中激活的时间冗余性，选择性上传显著变化的激活，结合自适应阈值控制与降维技术，大幅降低通信开销，同时保持模型性能。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.10564v2" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.10564v2" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Federated fine-tuning of on-device large language models (LLMs) mitigates privacy concerns by preventing raw data sharing. However, the intensive computational and memory demands pose significant challenges for resource-constrained edge devices. To overcome these limitations, split federated learning (SFL) emerges as a promising solution that partitions the model into lightweight client-side and compute-intensive server-side sub-models, thus offloading the primary training workload to a powerful server. Nevertheless, high-dimensional activation exchanges in SFL lead to excessive communication overhead. To overcome this, we propose SplitCom, a communication-efficient SFL framework for LLMs that exploits temporal redundancy in activations across consecutive training epochs. Inspired by video compression, the core innovation of our framework lies in selective activation uploading only when a noticeable deviation from previous epochs occurs. To balance communication efficiency and learning performance, we introduce two adaptive threshold control schemes based on 1) bang-bang control or 2) deep deterministic policy gradient (DDPG)-based reinforcement learning. Moreover, we implement dimensionality reduction techniques to alleviate client-side memory requirements. Furthermore, we extend SplitCom to the U-shape architecture, ensuring the server never accesses clients&#x27; labels. Extensive simulations and laboratory experiments demonstrate that SplitCom reduces uplink communication costs by up to 98.6\,\% in its standard configuration and total communication costs by up to 95.8\,\% in its U-shape variant without noticeably compromising model performance.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>在设备端大语言模型的联邦微调中，虽然避免了原始数据共享以保护隐私，但计算和内存需求对资源受限的边缘设备构成挑战。分割联邦学习通过将模型分割为客户端和服务器端子模型来减轻客户端负担，但高维激活交换导致通信开销过大。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出SplitCom框架，利用连续训练周期中激活的时间冗余性，仅当激活与先前周期相比出现显著偏差时才选择性上传。引入两种自适应阈值控制方案（基于bang-bang控制或DDPG强化学习）以平衡通信效率与学习性能，并采用降维技术降低客户端内存需求。还扩展了U形架构以确保服务器无法访问客户端标签。</p></div>
    <div class="detail-block"><h4>Result</h4><p>大量仿真和实验表明，SplitCom在标准配置下将上行通信成本降低高达98.6%，在U形变体中将总通信成本降低高达95.8%，且未显著损害模型性能。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>SplitCom通过有效减少分割联邦学习中的通信开销，为大语言模型在资源受限边缘设备上的高效、隐私保护微调提供了可行解决方案，在通信效率与模型性能间取得了良好平衡。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence</h3>
    <p class="paper-authors">Xiaoyue Ling, Chuqin Zhou, Chunyi Li, Yunuo Chen, Yuan Tian, Guo Lu, Wenjun Zhang</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-10</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出Free-GVC，一种免训练的生成式视频压缩框架，通过扩散先验压缩潜在轨迹，结合自适应质量控制和组间对齐模块，在超低码率下显著提升视觉质量和时间一致性。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.09868v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.09868v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有生成式视频压缩方法对时间相关性利用不足，在超低码率下易出现闪烁和时间连贯性下降的问题，需要改进以提升感知质量。</p></div>
    <div class="detail-block"><h4>Method</h4><p>采用基于视频扩散先验的潜在轨迹压缩，在GOP级别编码视频段；引入自适应质量控制模块动态预测最优扩散步长，并通过组间对齐模块进行潜在融合以减少闪烁。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，Free-GVC在DISTS指标上比最新神经编解码器DCVC-RT平均降低93.29%的BD-Rate，用户研究证实其在超低码率下具有更优的感知质量和时间连贯性。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>Free-GVC有效解决了超低码率下的闪烁和时间连贯性问题，为生成式视频压缩提供了高效且免训练的解决方案，显著提升了压缩视频的视觉体验。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 4</p>
    <h3 class="paper-title">Edge-Aligned Initialization of Kernels for Steered Mixture-of-Experts</h3>
    <p class="paper-authors">Martin Determann, Elvira Fleig</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-02</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了一种基于边缘检测的初始化方法，用于引导混合专家模型，显著减少了随机优化的需求，降低了计算和内存成本，同时保持了良好的图像重建质量。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.02031v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.02031v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Steered Mixture-of-Experts (SMoE) has recently emerged as a powerful framework for spatial-domain image modeling, enabling high-fidelity image representation using a remarkably small number of parameters. Its ability to steer kernel-based experts toward structural image features has led to successful applications in image compression, denoising, super-resolution, and light field processing. However, practical adoption is hindered by the reliance on gradient-based optimization to estimate model parameters on a per-image basis - a process that is computationally intensive and difficult to scale. Initialization strategies for SMoE are an essential component that directly affects convergence and reconstruction quality. In this paper, we propose a novel, edge-based initialization scheme that achieves good reconstruction qualities while reducing the need for stochastic optimization significantly. Through a method that leverages Canny edge detection to extract a sparse set of image contours, kernel positions and orientations are deterministically inferred. A separate approach enables the direct estimation of initial expert coefficients. This initialization reduces both memory consumption and computational cost.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>引导混合专家模型在图像处理中表现出色，但其参数估计依赖计算密集、难以扩展的逐图像梯度优化，初始化策略直接影响收敛和重建质量，因此需要一种高效初始化方法来克服这些瓶颈。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出一种基于Canny边缘检测的初始化方案：通过提取稀疏图像轮廓，确定性地推断核位置和方向，并直接估计专家系数初始值，从而减少对随机优化的依赖。</p></div>
    <div class="detail-block"><h4>Result</h4><p>该方法在减少内存消耗和计算成本的同时，实现了良好的重建质量，显著降低了优化过程的计算负担。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>基于边缘的初始化方案有效解决了引导混合专家模型在实用化中的计算可扩展性问题，为高效图像建模提供了可行路径。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 5</p>
    <h3 class="paper-title">AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems</h3>
    <p class="paper-authors">Rajat Bhattacharjya, Sing-Yao Wu, Hyunwoo Oh, Chaewon Nam, Suyeon Koo, Mohsen Imani, Elaheh Bozorgzadeh, Nikil Dutt</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-02</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.DC</strong> · cs.AR · cs.CV · cs.LG · cs.NI</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出AVERY框架，通过自适应分载计算在无人机上部署视觉语言模型，解决灾难响应中资源受限与低带宽网络下的实时语义推理问题。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2511.18151v2" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2511.18151v2" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution &quot;context stream&quot; for real-time awareness and a low-frequency, high-fidelity &quot;insight stream&quot; for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>无人机在灾难响应中需要可查询的语义智能，但机上CNN无法提供，而视觉语言模型资源需求高，难以在设备端部署；云卸载在低带宽灾难网络中又不可行。</p></div>
    <div class="detail-block"><h4>Method</h4><p>AVERY采用认知启发的双流分载方法：高频低分辨率&#x27;上下文流&#x27;用于实时感知，低频高保真&#x27;洞察流&#x27;用于深度分析；轻量控制器根据网络状态和操作意图动态选择压缩模型，平衡精度与吞吐。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在边缘-云场景下使用LISA-7B模型测试，AVERY比静态配置表现更优：比原始图像压缩精度高11.2%，比全边缘执行能耗低93.98%。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>AVERY框架通过自适应分载计算，实现了资源受限平台上动态环境中的实时可查询智能，提升了任务效率。</p></div>
  </details>
</article>
  </div>
</section>

<section id="query-lic" class="query-section" data-query-label="LIC 端到端" data-paper-count="5">
  <header class="query-header">
    <h2>LIC 端到端</h2>
    <p class="query-count">5 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation</h3>
    <p class="paper-authors">Lingyong Yan, Jiulong Wu, Dong Xie, Weixian Shi, Deguo Xia, Jizhou Huang</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-12</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.AI</strong> · cs.CL</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出LAVES，一个基于LLM的多智能体分层系统，用于从教育问题自动生成高质量教学视频。它通过分解工作流、质量门控和可执行脚本编译，实现了高逻辑严谨性、低成本和高吞吐量。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.11790v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.11790v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio--visual alignment. To address the limitations of prior approaches--including low procedural fidelity, high production cost, and limited controllability--LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有端到端视频生成模型在需要严格逻辑和精确知识表示的场景（如教学视频）中存在局限，表现为程序保真度低、生产成本高、可控性有限。</p></div>
    <div class="detail-block"><h4>Method</h4><p>LAVES采用分层多智能体框架：一个编排智能体协调解决方案智能体（严谨解题）、插图智能体（生成可视化代码）和旁白智能体（教学脚本），并引入语义批判、规则约束和工具检查。系统生成结构化可执行视频脚本，通过模板驱动规则编译为同步的视听内容，实现全自动端到端生产。</p></div>
    <div class="detail-block"><h4>Result</h4><p>大规模部署中，LAVES每日吞吐量超百万视频，相比行业标准方法成本降低95%以上，同时保持高接受率。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>LAVES通过多智能体协作与结构化编译方法，有效解决了教学视频生成中对逻辑严谨性、教育连贯性和成本效率的需求，为高质量教育媒体自动化生产提供了可行方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition</h3>
    <p class="paper-authors">Yihan Hu, Xuelin Chen, Xiaodong Cun</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-26</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出EasyOmnimatte，一种基于视频修复扩散模型的端到端视频全遮罩方法，通过双专家策略（效果专家与质量专家）在去噪过程的不同阶段协作，实现高质量前景层与关联效果的分解，显著提升了效率与质量。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.21865v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.21865v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Existing video omnimatte methods typically rely on slow, multi-stage, or inference-time optimization pipelines that fail to fully exploit powerful generative priors, producing suboptimal decompositions. Our key insight is that, if a video inpainting model can be finetuned to remove the foreground-associated effects, then it must be inherently capable of perceiving these effects, and hence can also be finetuned for the complementary task: foreground layer decomposition with associated effects. However, although naïvely finetuning the inpainting model with LoRA applied to all blocks can produce high-quality alpha mattes, it fails to capture associated effects. Our systematic analysis reveals this arises because effect-related cues are primarily encoded in specific DiT blocks and become suppressed when LoRA is applied across all blocks. To address this, we introduce EasyOmnimatte, the first unified, end-to-end video omnimatte method. Concretely, we finetune a pretrained video inpainting diffusion model to learn dual complementary experts while keeping its original weights intact: an Effect Expert, where LoRA is applied only to effect-sensitive DiT blocks to capture the coarse structure of the foreground and associated effects, and a fully LoRA-finetuned Quality Expert learns to refine the alpha matte. During sampling, Effect Expert is used for denoising at early, high-noise steps, while Quality Expert takes over at later, low-noise steps. This design eliminates the need for two full diffusion passes, significantly reducing computational cost without compromising output quality. Ablation studies validate the effectiveness of this Dual-Expert strategy. Experiments demonstrate that EasyOmnimatte sets a new state-of-the-art for video omnimatte and enables various downstream tasks, significantly outperforming baselines in both quality and efficiency.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有视频全遮罩方法通常依赖缓慢、多阶段或推理时优化的流程，未能充分利用强大的生成先验，导致分解效果不佳。本文旨在开发一种统一、高效的端到端方法，以克服这些限制。</p></div>
    <div class="detail-block"><h4>Method</h4><p>方法基于预训练视频修复扩散模型，通过微调学习双专家：效果专家（仅在效果敏感的DiT块应用LoRA）捕获前景与关联效果的粗结构；质量专家（全块LoRA微调）细化alpha遮罩。采样时，效果专家用于早期高噪声步骤，质量专家用于后期低噪声步骤，无需两次完整扩散过程。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，EasyOmnimatte在视频全遮罩任务上实现了新的最优性能，在质量和效率上均显著超越基线方法，并能支持多种下游任务。消融研究验证了双专家策略的有效性。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>EasyOmnimatte通过创新的双专家设计，首次提供了统一、端到端的视频全遮罩解决方案，在保持高质量输出的同时大幅降低了计算成本，为相关应用提供了高效工具。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation</h3>
    <p class="paper-authors">Daichi Arai, Yuichi Kondo, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-23</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">eess.IV</strong> · cs.MM</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出一种无需重新训练或修改模型架构的实用方法，通过将传统视频编码中的量化参数自适应技术扩展至神经视频压缩模型，结合等距柱状投影的空间变化采样密度，实现了360度视频的高效压缩。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.20093v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.20093v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>This study proposes a practical approach for compressing 360-degree equirectangular videos using pretrained neural video compression (NVC) models. Without requiring additional training or changes in the model architectures, the proposed method extends quantization parameter adaptation techniques from traditional video codecs to NVC, utilizing the spatially varying sampling density in equirectangular projections. We introduce latitude-based adaptive quality parameters through rate-distortion optimization for NVC. The proposed method utilizes vector bank interpolation for latent modulation, enabling flexible adaptation with arbitrary quality parameters and mitigating the limitations caused by rounding errors in the adaptive quantization parameters. Experimental results demonstrate that applying this method to the DCVC-RT framework yields BD-Rate savings of 5.2% in terms of the weighted spherical peak signal-to-noise ratio for JVET class S1 test sequences, with only a 0.3% increase in processing time.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>360度等距柱状投影视频在压缩时，由于投影本身的空间采样密度不均匀（如两极区域过度采样），直接使用现有神经视频压缩模型会导致比特率分配效率低下。因此，需要一种无需额外训练或架构改动的方法，自适应地调整压缩质量以提升整体率失真性能。</p></div>
    <div class="detail-block"><h4>Method</h4><p>方法基于预训练的神经视频压缩模型，引入纬度自适应的质量参数，通过率失真优化确定参数值。利用矢量库插值进行潜在表示调制，以灵活适应任意质量参数，并减少自适应量化参数舍入误差带来的限制。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在DCVC-RT框架上应用该方法，对JVET S1类测试序列的实验结果显示，在加权球面峰值信噪比指标上实现了5.2%的BD-Rate节省，同时处理时间仅增加0.3%。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>所提出的方法有效提升了神经视频压缩模型在360度视频上的压缩效率，证明了量化参数自适应技术与神经压缩模型结合的可行性，且计算开销极小，具有实际应用价值。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 4</p>
    <h3 class="paper-title">FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</h3>
    <p class="paper-authors">Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-18</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出FlashPortrait，一种端到端视频扩散Transformer，通过身份无关特征提取、归一化对齐、动态滑动窗口和高阶潜在导数预测，实现身份一致的长肖像动画生成，推理速度提升高达6倍。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.16900v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.16900v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>当前基于扩散模型的长肖像动画加速方法难以保证身份一致性，需要一种能同时保持身份稳定并加速推理的解决方案。</p></div>
    <div class="detail-block"><h4>Method</h4><p>使用现成提取器获取身份无关的面部表情特征；引入归一化面部表情块对齐特征与扩散潜在表示；采用动态滑动窗口加权融合确保过渡平滑；基于潜在变化率和导数幅度比，利用高阶潜在导数跳过降噪步骤以加速推理。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在基准测试中，FlashPortrait在定性和定量上均有效，能合成身份保持的无限长视频，推理速度提升高达6倍。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>FlashPortrait通过创新设计解决了长肖像动画中的身份一致性和速度瓶颈，为高质量视频合成提供了高效端到端方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 5</p>
    <h3 class="paper-title">Content Adaptive based Motion Alignment Framework for Learned Video Compression</h3>
    <p class="paper-authors">Tiange Zhang, Xiandong Meng, Siwei Ma</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-15</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.CV</strong> · cs.AI</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了一种内容自适应的运动对齐框架（CAMA），通过两阶段流引导可变形扭曲、多参考质量感知策略和免训练下采样模块，显著提升了神经视频压缩性能，在BD-rate上实现了24.95%的节省。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.12936v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.12936v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有的端到端视频压缩框架缺乏内容特异性适应，导致压缩性能次优。本文旨在通过内容自适应策略，针对不同视频内容特性优化编码，以提升压缩效率。</p></div>
    <div class="detail-block"><h4>Method</h4><p>方法包括：1）两阶段流引导可变形扭曲机制，通过粗到细的偏移预测和掩码调制实现精确特征对齐；2）多参考质量感知策略，根据参考质量调整失真权重，并结合分层训练减少误差传播；3）集成免训练模块，根据运动幅度和分辨率下采样帧以获取平滑运动估计。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在标准测试数据集上，CAMA框架显著优于现有神经视频压缩模型，相比基线模型DCVC-TCM实现了24.95%的BD-rate（PSNR）节省，同时优于复现的DCVC-DC和传统编解码器HM-16.25。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>所提出的内容自适应运动对齐框架通过针对性优化运动补偿和参考处理，有效提升了视频压缩性能，证明了内容适应在端到端学习中的重要性。</p></div>
  </details>
</article>
  </div>
</section>

<section id="query-query-2" class="query-section" data-query-label="视频质量指标" data-paper-count="5">
  <header class="query-header">
    <h2>视频质量指标</h2>
    <p class="query-count">5 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals</h3>
    <p class="paper-authors">Yu-Chih Chen, Michael Wang, Chieh-Dun Wen, Kai-Siang Ma, Avinab Saha, Li-Heng Chen, Alan Bovik</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-12</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">eess.IV</strong> · cs.CV · cs.MM</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出MTL-VQA，一种多任务学习框架，利用全参考指标作为监督信号进行预训练，无需人工标注，以解决游戏视频无参考质量评估中数据稀缺和内容独特性的挑战。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.11903v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.11903v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>No-reference video quality assessment (NR-VQA) for gaming videos is challenging due to limited human-rated datasets and unique content characteristics including fast motion, stylized graphics, and compression artifacts. We present MTL-VQA, a multi-task learning framework that uses full-reference metrics as supervisory signals to learn perceptually meaningful features without human labels for pretraining. By jointly optimizing multiple full-reference (FR) objectives with adaptive task weighting, our approach learns shared representations that transfer effectively to NR-VQA. Experiments on gaming video datasets show MTL-VQA achieves performance competitive with state-of-the-art NR-VQA methods across both MOS-supervised and label-efficient/self-supervised settings.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>游戏视频的无参考质量评估面临两大挑战：一是人工评分数据集有限，二是游戏视频具有快速运动、风格化图形和压缩伪影等独特内容特征，使得传统方法难以有效处理。</p></div>
    <div class="detail-block"><h4>Method</h4><p>采用多任务学习框架，通过联合优化多个全参考指标目标并自适应调整任务权重，学习可迁移的共享特征表示，从而在无参考质量评估任务中实现有效知识迁移。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在游戏视频数据集上的实验表明，MTL-VQA在MOS监督、标签高效及自监督设置下，均能达到与当前最先进无参考质量评估方法竞争的性能。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>MTL-VQA通过多任务学习利用全参考指标作为代理监督信号，成功克服了游戏视频质量评估中数据标注的局限性，为无参考质量评估提供了一种有效的预训练策略。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">SCRAPL: Scattering Transform with Random Paths for Machine Learning</h3>
    <p class="paper-authors">Christopher Mitcheltree, Vincent Lostanlen, Emmanouil Benetos, Mathieu Lagrange</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-11</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.SD</strong> · cs.LG · eess.AS</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>提出SCRAPL方法，通过随机路径采样高效计算小波散射变换，解决其在神经网络训练中计算成本高的问题，应用于DDSP声音匹配任务，并引入重要性采样初始化提升性能。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.11145v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.11145v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose &quot;Scattering transform with Random Paths for machine Learning&quot; (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>小波散射变换在感知质量评估中提供有效梯度，但因其路径众多、计算昂贵，难以作为可微损失函数用于随机梯度下降，限制了在神经网络训练中的应用。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出SCRAPL（随机路径散射变换），一种随机优化方案，通过采样随机路径子集来高效近似多变量散射变换；针对联合时频散射变换（JTFS）实现，并引入基于重要性采样的初始化启发式方法，以适应数据集的感知内容。</p></div>
    <div class="detail-block"><h4>Result</h4><p>将SCRAPL应用于可微分数字信号处理（DDSP），在颗粒合成器和Roland TR-808鼓机的声音匹配任务中验证了其有效性；重要性采样初始化提高了神经网络收敛速度和评估性能。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>SCRAPL通过随机路径采样显著降低了散射变换的计算成本，使其适用于神经网络训练，并在音频处理任务中展示了实用性和性能提升，提供了开源代码和Python包。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">OnlineVPO: Align Video Diffusion Model with Online Video-Centric Preference Optimization</h3>
    <p class="paper-authors">Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-31</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出OnlineVPO，一种针对视频扩散模型（VDMs）的高效偏好学习框架。通过使用视频质量评估（VQA）模型作为人类偏好的代理，并设计在线DPO算法，解决了现有方法因图像级奖励模型模态不匹配和离线学习导致的视频质量下降、闪烁及优化不足问题，提升了生成视频的质量和稳定性。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2412.15159v2" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2412.15159v2" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Video diffusion models (VDMs) have demonstrated remarkable capabilities in text-to-video (T2V) generation. Despite their success, VDMs still suffer from degraded image quality and flickering artifacts. To address these issues, some approaches have introduced preference learning to exploit human feedback to enhance the video generation. However, these methods primarily adopt the routine in the image domain without an in-depth investigation into video-specific preference optimization. In this paper, we reexamine the design of the video preference learning from two key aspects: feedback source and feedback tuning methodology, and present OnlineVPO, a more efficient preference learning framework tailored specifically for VDMs. On the feedback source, we found that the image-level reward model commonly used in existing methods fails to provide a human-aligned video preference signal due to the modality gap. In contrast, video quality assessment (VQA) models show superior alignment with human perception of video quality. Building on this insight, we propose leveraging VQA models as a proxy of humans to provide more modality-aligned feedback for VDMs. Regarding the preference tuning methodology, we introduce an online DPO algorithm tailored for VDMs. It not only enjoys the benefits of superior scalability in optimizing videos with higher resolution and longer duration compared with the existing method, but also mitigates the insufficient optimization issue caused by off-policy learning via online preference generation and curriculum preference update designs. Extensive experiments on the open-source video-diffusion model demonstrate OnlineVPO as a simple yet effective and, more importantly, scalable preference learning algorithm for video diffusion models.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>当前视频扩散模型（VDMs）在文本到视频生成中仍存在图像质量下降和闪烁伪影问题。现有偏好学习方法大多直接沿用图像域技术，未深入探究视频特有的偏好优化，导致反馈信号与视频模态不匹配，且离线学习方式可能造成优化不足。</p></div>
    <div class="detail-block"><h4>Method</h4><p>1. 反馈源：采用视频质量评估（VQA）模型替代图像级奖励模型，提供与人类感知更一致、模态对齐的视频偏好信号。2. 调优方法：提出针对VDMs的在线DPO算法，通过在线偏好生成和课程偏好更新设计，支持更高分辨率、更长视频的优化，并缓解离线学习的优化不足问题。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在开源视频扩散模型上的大量实验表明，OnlineVPO能有效提升生成视频的质量和稳定性，减少闪烁，且算法简单、高效、可扩展，优于现有偏好学习方法。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>OnlineVPO是一个专为视频扩散模型设计的有效偏好学习框架，通过视频对齐的反馈源和在线优化方法，显著改善了视频生成质量，为VDMs的偏好优化提供了更适配的解决方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 4</p>
    <h3 class="paper-title">Image and Video Quality Assessment using Prompt-Guided Latent Diffusion Models for Cross-Dataset Generalization</h3>
    <p class="paper-authors">Shankhanil Mitra, Diptanu De, Shika Rao, Rajiv Soundararajan</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-28</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">eess.IV</strong> · cs.LG</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出一种基于扩散模型去噪过程的图像与视频质量评估方法，通过学习质量感知文本提示与视觉内容的对齐程度，结合时间质量调制器补偿帧采样损失，实现了在多种数据集上的优异泛化性能。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2406.04654v2" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2406.04654v2" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>The design of image and video quality assessment (QA) algorithms is extremely important to benchmark and calibrate user experience in modern visual systems. A major drawback of the state-of-the-art QA methods is their limited ability to generalize across diverse image and video datasets with reasonable distribution shifts. In this work, we leverage the denoising process of diffusion models for generalized image QA (IQA) and video QA (VQA) by understanding the degree of alignment between learnable quality-aware text prompts and images or video frames. In particular, we learn cross-attention maps from intermediate layers of the denoiser of latent diffusion models (LDMs) to capture quality-aware representations of images or video frames. Since applying text-to-image LDMs for every video frame is computationally expensive for videos, we only estimate the quality of a frame-rate sub-sampled version of the original video. To compensate for the loss in motion information due to frame-rate sub-sampling, we propose a novel temporal quality modulator. Our extensive cross-database experiments across various user-generated, synthetic, low-light, frame-rate variation, ultra high definition, and streaming content-based databases show that our model can achieve superior generalization in both IQA and VQA.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有质量评估方法在跨数据集泛化能力上存在局限，难以适应分布差异较大的图像与视频数据，需要一种更通用的评估框架。</p></div>
    <div class="detail-block"><h4>Method</h4><p>利用潜在扩散模型的去噪器中间层学习交叉注意力图，捕获图像或视频帧的质量感知表征；对视频进行帧率下采样以降低计算成本，并设计时间质量调制器补偿运动信息损失。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在用户生成内容、合成、低光照、帧率变化、超高清及流媒体等多种数据库上的跨库实验表明，该方法在图像与视频质量评估中均实现了优越的泛化能力。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>基于扩散模型对齐学习的质量评估方法能有效提升跨数据集泛化性能，为图像与视频系统的用户体验校准提供了通用解决方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 5</p>
    <h3 class="paper-title">Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</h3>
    <p class="paper-authors">Xinhao Xiang, Abhijeet Rastogi, Jiawei Zhang</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-06</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出一个诊断框架，评估AI生成驾驶视频（AIGV）用于自动驾驶模型训练与测试的可靠性。研究发现原始AIGV存在多种缺陷会损害感知性能，但通过提出的ADGVE评估器进行过滤后，AIGV可成为真实数据的有益补充。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.06376v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.06376v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>文本到视频模型能生成高分辨率驾驶场景，为自动驾驶提供低成本、可扩展的数据替代方案，但其能否可靠支持自动驾驶模型的训练与评估尚不明确，需系统研究其潜在风险与价值。</p></div>
    <div class="detail-block"><h4>Method</h4><p>首先建立AIGV常见故障模式的分类法（如视觉伪影、物理运动不合理、交通语义违规），构建带人工标注的驾驶基准测试集ADGV-Bench；然后提出ADGVE评估器，融合静态语义、时序线索、车道遵守信号和视觉语言模型推理，为每个视频片段生成综合质量分数。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，盲目使用原始AIGV会降低感知模型性能；而经ADGVE过滤后，不仅能提升通用视频质量评估指标，还能改善下游自动驾驶模型表现，使AIGV成为真实数据的有效补充。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>研究揭示了AIGV在自动驾驶应用中的风险与潜力，并提供了ADGVE等实用工具，为安全利用大规模视频生成技术提供了方法论支持。</p></div>
  </details>
</article>
  </div>
</section>

<section id="query-vsr" class="query-section" data-query-label="VSR 视频超分辨率" data-paper-count="5">
  <header class="query-header">
    <h2>VSR 视频超分辨率</h2>
    <p class="query-count">5 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos</h3>
    <p class="paper-authors">Kavitha Viswanathan, Vrinda Goel, Shlesh Gholap, Devayan Ghosh, Madhav Gupta, Dhruvi Ganatra, Sanket Potdar, Amit Sethi</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-10</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了FANVID视频基准数据集，用于评估低分辨率视频中的人脸匹配与车牌识别任务，强调利用时序信息解决单帧无法识别的问题，并提供了基线方法与评估指标。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2506.07304v2" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2506.07304v2" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Real-world surveillance often renders faces and license plates unrecognizable in individual low-resolution (LR) frames, hindering reliable identification. To advance temporal recognition models, we present FANVID, a novel video-based benchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63 identities and 49 license plates from three English-speaking countries. Each video includes distractor faces and plates, increasing task difficulty and realism. The dataset contains 31,096 manually verified bounding boxes and labels.
  FANVID defines two tasks: (1) face matching -- detecting LR faces and matching them to high-resolution mugshots, and (2) license plate recognition -- extracting text from LR plates without a predefined database. Videos are downsampled from high-resolution sources to ensure that faces and text are indecipherable in single frames, requiring models to exploit temporal information. We introduce evaluation metrics adapted from mean Average Precision at IoU &gt; 0.5, prioritizing identity correctness for faces and character-level accuracy for text.
  A baseline method with pre-trained video super-resolution, detection, and recognition achieved performance scores of 0.58 (face matching) and 0.42 (plate recognition), highlighting both the feasibility and challenge of the tasks. FANVID&#x27;s selection of faces and plates balances diversity with recognition challenge. We release the software for data access, evaluation, baseline, and annotation to support reproducibility and extension. FANVID aims to catalyze innovation in temporal modeling for LR recognition, with applications in surveillance, forensics, and autonomous vehicles.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现实监控视频中的人脸和车牌常因低分辨率在单帧中无法识别，现有方法依赖单帧信息，缺乏利用视频时序信息进行可靠识别的基准，限制了时序识别模型的发展。</p></div>
    <div class="detail-block"><h4>Method</h4><p>创建FANVID数据集，包含1,463个低分辨率视频片段，涉及63个人物和49个车牌，并添加干扰项以提升任务难度；定义人脸匹配和车牌识别两项任务，采用从高分辨率视频下采样的方式确保单帧不可识别；提出基于交并比&gt;0.5的平均精度均值等评估指标，并提供了结合视频超分辨率、检测与识别的基线方法。</p></div>
    <div class="detail-block"><h4>Result</h4><p>基线方法在FANVID上取得了人脸匹配任务0.58分和车牌识别任务0.42分的性能，表明任务具有可行性但挑战性较高，验证了数据集对推动时序建模研究的价值。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>FANVID数据集通过强调时序信息利用，为低分辨率视频识别任务提供了标准化基准，有望推动监控、取证和自动驾驶等领域的时序模型创新，并开源了相关软件以支持可复现性和扩展研究。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution</h3>
    <p class="paper-authors">Tianxing Wu, Zheng Chen, Cirou Xu, Bowen Chai, Yong Guo, Yutong Liu, Linghe Kong, Yulun Zhang</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-03</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>提出LSGQuant方法，通过动态范围自适应量化器、层敏感度引导和量化感知优化，高效压缩一步扩散视频超分辨率模型，在保持性能的同时显著降低计算成本。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.03182v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.03182v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: https://github.com/zhengchen1999/LSGQuant.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>一步扩散模型在视频超分辨率中性能优异但计算开销大，传统量化方法因输入动态范围高和层行为多样而效果受限，需针对性解决以推动实际应用。</p></div>
    <div class="detail-block"><h4>Method</h4><p>引入LSGQuant：1) 动态范围自适应量化器(DRAQ)适应视频token激活值；2) 基于校准统计的层敏感度估计和方差导向层训练策略(VOLTS)；3) 量化感知优化(QAO)联合微调量化分支和高精度分支。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，该方法在量化后性能接近全精度原模型，显著优于现有量化技术，实现了高效模型压缩。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>LSGQuant通过自适应量化和层敏感度指导，有效平衡了一步扩散视频超分辨率模型的性能与效率，为实际部署提供了可行方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">FedVSR: Towards Model-Agnostic Federated Learning in Video Super-Resolution</h3>
    <p class="paper-authors">Ali Mollaahmadi Dehaghi, Hossein KhademSohi, Reza Razavi, Steve Drew, Mohammad Moshirpour</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-02</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.CV</strong> · cs.DC</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了首个针对视频超分辨率（VSR）的联邦学习框架FedVSR，通过引入基于离散小波变换的轻量级损失函数和损失感知聚合策略，在保护隐私的同时显著提升了视频质量，且计算与通信开销近乎为零。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2503.13745v3" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2503.13745v3" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Video super-resolution (VSR) aims to enhance low-resolution videos by leveraging both spatial and temporal information. While deep learning has led to impressive progress, it typically requires centralized data, which raises privacy concerns. Federated learning (FL) offers a privacy-friendly solution, but general FL frameworks often struggle with low-level vision tasks, resulting in blurry, low-quality outputs. To address this, we introduce FedVSR, the first FL framework specifically designed for VSR. It is model-agnostic and stateless, and introduces a lightweight loss function based on the Discrete Wavelet Transform (DWT) to better preserve high-frequency details during local training. Additionally, a loss-aware aggregation strategy combines both DWT-based and task-specific losses to guide global updates effectively. Extensive experiments across multiple VSR models and datasets show that FedVSR not only improves perceptual video quality (up to +0.89 dB PSNR, +0.0370 SSIM, -0.0347 LPIPS and 4.98 VMAF) but also achieves these gains with close to zero computation and communication overhead compared to its rivals. These results demonstrate FedVSR&#x27;s potential to bridge the gap between privacy, efficiency, and perceptual quality, setting a new benchmark for federated learning in low-level vision tasks. The code is available at: https://github.com/alimd94/FedVSR</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>视频超分辨率通常依赖集中式数据训练深度学习模型，这引发了隐私担忧。联邦学习虽能保护隐私，但现有通用框架在处理VSR等低级视觉任务时效果不佳，常产生模糊、低质量的输出。因此，需要一种专门为VSR设计的联邦学习解决方案。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出了FedVSR框架，它是模型无关且无状态的。核心创新包括：1）引入基于离散小波变换（DWT）的轻量级损失函数，以在本地训练中更好地保留高频细节；2）采用损失感知聚合策略，结合DWT损失和任务特定损失来有效指导全局模型更新。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在多个VSR模型和数据集上的广泛实验表明，FedVSR显著提升了感知视频质量（PSNR最高提升+0.89 dB，SSIM提升+0.0370，LPIPS降低-0.0347，VMAF提升4.98）。与现有方法相比，它在实现这些提升的同时，计算和通信开销近乎为零。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>FedVSR成功地在隐私保护、效率和感知质量之间架起了桥梁，为联邦学习在低级视觉任务中的应用设立了新基准，证明了其在实际部署中的潜力。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 4</p>
    <h3 class="paper-title">Zero-Shot Video Restoration and Enhancement with Assistance of Video Diffusion Models</h3>
    <p class="paper-authors">Cong Cao, Huanjing Yue, Shangbin Xie, Xin Liu, Jingyu Yang</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-01-29</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>提出首个利用视频扩散模型辅助基于图像的零样本视频修复与增强方法，以解决时序闪烁问题，无需训练，可泛化至各类图像方法。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2601.21922v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2601.21922v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Although diffusion-based zero-shot image restoration and enhancement methods have achieved great success, applying them to video restoration or enhancement will lead to severe temporal flickering. In this paper, we propose the first framework that utilizes the rapidly-developed video diffusion model to assist the image-based method in maintaining more temporal consistency for zero-shot video restoration and enhancement. We propose homologous latents fusion, heterogenous latents fusion, and a COT-based fusion ratio strategy to utilize both homologous and heterogenous text-to-video diffusion models to complement the image method. Moreover, we propose temporal-strengthening post-processing to utilize the image-to-video diffusion model to further improve temporal consistency. Our method is training-free and can be applied to any diffusion-based image restoration and enhancement methods. Experimental results demonstrate the superiority of the proposed method.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有基于扩散的零样本图像修复与增强方法应用于视频时会产生严重时序闪烁，缺乏保持时序一致性的有效方案。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出框架融合同源与异源文本到视频扩散模型以补充图像方法，包括同源/异源潜在融合、COT融合比策略，并使用时序强化后处理进一步改善一致性。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验结果表明该方法在保持时序一致性方面优于现有方法，能有效提升视频修复与增强质量。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>所提训练无关框架能有效利用视频扩散模型增强图像方法的时序一致性，为零样本视频修复与增强提供了通用解决方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 5</p>
    <h3 class="paper-title">Curriculum-Learned Vanishing Stacked Residual PINNs for Hyperbolic PDE State Reconstruction</h3>
    <p class="paper-authors">Katayoun Eshkofti, Matthieu Barreau</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-01-28</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.NE</strong> · cs.LG · eess.SY</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了一种结合三种课程学习方法的VSR-PINN，用于改进双曲偏微分方程建模，通过优化损失平衡、因果时序和自适应采样，显著提升了交通重建任务的精度和稳定性。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.06996v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.06996v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Modeling distributed dynamical systems governed by hyperbolic partial differential equations (PDEs) remains challenging due to discontinuities and shocks that hinder the convergence of traditional physics-informed neural networks (PINNs). The recently proposed vanishing stacked residual PINN (VSR-PINN) embeds a vanishing-viscosity mechanism within stacked residual refinements to enable a smooth transition from the parabolic to hyperbolic regime. This paper integrates three curriculum-learning methods as primal-dual (PD) optimization, causality progression, and adaptive sampling into the VSR-PINN. The PD strategy balances physics and data losses, the causality scheme unlocks deeper stacks by respecting temporal and gradient evolution, and adaptive sampling targets high residuals. Numerical experiments on traffic reconstruction confirm that enforcing causality systematically reduces the median point-wise MSE and its variability across runs, yielding improvements of nearly one order of magnitude over non-causal training in both the baseline and PD variants.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>传统物理信息神经网络在处理具有间断和激波的双曲偏微分方程时面临收敛困难，而现有的VSR-PINN方法虽引入粘性机制，但仍需进一步提升训练效率和精度。</p></div>
    <div class="detail-block"><h4>Method</h4><p>将三种课程学习方法集成到VSR-PINN中：原始-对偶优化平衡物理与数据损失，因果推进策略尊重时序和梯度演化以解锁更深网络堆叠，自适应采样针对高残差区域进行优化。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在交通重建数值实验中，因果训练显著降低了中值点均方误差及其跨运行变异性，相比非因果训练在基线和原始-对偶变体中均提升近一个数量级。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>集成课程学习的VSR-PINN能有效提升双曲PDE建模的精度和鲁棒性，因果策略是关键改进因素，为分布式动力系统仿真提供了更可靠的框架。</p></div>
  </details>
</article>
  </div>
</section>
    </main>
  </div>

  <script src="./assets/interactive.js"></script>
</body>
</html>
