<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Paper Tracker Daily Reading</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:wght@400;700&family=Crimson+Pro:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="./assets/style.css">
</head>
<body class="theme-light">
  <a class="skip-link" href="#main-content">跳转到正文</a>

  <header class="page-header">
    <div class="header-main">
      <p class="header-topline">
        POWER BY
        <a class="powered-by-link" href="https://github.com/RainerSeventeen/paper-tracker" target="_blank" rel="noopener noreferrer">
          RainerSeventeen/paper-tracker
        </a>
      </p>
      <h1>文献阅读简报</h1>
      <p class="header-meta">生成时间：2026-02-24 10:39:04</p>
      <div class="report-stats">
        <p><span>3</span> 个查询主题</p>
        <p><span>9</span> 篇候选论文</p>
      </div>
    </div>
    <div class="header-actions">
      <button id="expand-all" type="button" aria-label="展开所有论文详情">展开全部</button>
      <button id="collapse-all" type="button" aria-label="折叠所有论文详情">折叠全部</button>
      <button id="theme-toggle" type="button" aria-label="切换暗色模式">切换暗色模式</button>
    </div>
  </header>

  <div class="layout">
    <aside id="sidebar" class="sidebar" aria-label="查询导航">
      <p class="sidebar-title">文献目录</p>
      <p class="sidebar-note">按查询主题快速定位</p>
    </aside>
    <main id="main-content" class="content">
      <section id="query-query" class="query-section" data-query-label="视频及图像压缩" data-paper-count="3">
  <header class="query-header">
    <h2>视频及图像压缩</h2>
    <p class="query-count">3 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">Convolutional Optical Encoders for Generalizable Image Compression</h3>
    <p class="paper-authors">Yubo Zhang, Rui Chen, Zhihao Zhou, Arka Majumdar</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-20</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>研究利用超光学编码器的平移不变点扩散函数实现可泛化的图像压缩，在相同压缩比下，空间分块编码重建质量最高但抗噪性较差，多通道方法更稳健。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.18032v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.18032v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>We investigate the utility of meta-optical encoders for generalizable image compression by leveraging their intrinsic shift-invariant point spread functions (PSFs). Compared with purely digital approaches, such optical encoders offer parallel and energy-efficient compression, enabling early data reduction prior to electronic processing and transmission, which is particularly attractive for resource-constrained and compact imaging systems. Although the operations realizable by a single passive optical layer remain fundamentally constrained, we systematically study several PSF encoding strategies combined with a total-variation (TV) digital reconstruction backend. Specifically, under identical compression ratios, we compare spatial binning, multi-channel random, and multi-channel orthogonal PSF based designs. Our results show that, at the same compression ratios, spatial binning achieves the highest reconstruction quality among all encoding strategies; however, it exhibits limited robustness to noise compared with multi-channel methods.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>相比纯数字方法，光学编码器能提供并行、节能的压缩，在电子处理与传输前实现早期数据降维，适用于资源受限的紧凑成像系统。</p></div>
    <div class="detail-block"><h4>Method</h4><p>系统研究多种点扩散函数编码策略（空间分块、多通道随机、多通道正交）与全变分数字重建后端结合，在相同压缩比下进行比较。</p></div>
    <div class="detail-block"><h4>Result</h4><p>相同压缩比下，空间分块编码重建质量最高，但抗噪性弱于多通道方法；多通道编码策略在噪声环境下更具鲁棒性。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>超光学编码器在图像压缩中具有应用潜力，空间分块编码适合高质量重建场景，而多通道方法在抗噪要求高的环境中更优。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos</h3>
    <p class="paper-authors">Namitha Padmanabhan, Matthew Gwilliam, Abhinav Shrivastava</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-18</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出TeCoNeRV，一种基于超网络的视频压缩方法，通过空间-时间分解、残差存储和时间相干性正则化，显著降低了内存开销和码率，提升了高分辨率视频的压缩质量和编码速度。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.16711v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.16711v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>隐式神经表示（INR）在视频压缩中表现优异，但为每个视频单独训练INR难以扩展到高分辨率，且现有超网络方法存在质量低、压缩尺寸大、内存需求高的问题。</p></div>
    <div class="detail-block"><h4>Method</h4><p>方法包括：（1）将视频片段分解为时空块，降低预训练内存开销；（2）采用残差存储方案，仅保存连续片段间的差异以减少码流；（3）引入时间相干性正则化，使权重变化与视频内容相关。</p></div>
    <div class="detail-block"><h4>Result</h4><p>在UVG数据集上，480p和720p分辨率下PSNR分别提升2.47dB和5.35dB，码率降低36%，编码速度加快1.5-3倍，首次在480p、720p和1080p分辨率上展示了超网络方法的可行结果。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>TeCoNeRV通过创新设计解决了超网络视频压缩的内存、质量和效率瓶颈，为高分辨率视频压缩提供了高效且可扩展的解决方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">SEG-JPEG: Simple Visual Semantic Communications for Remote Operation of Automated Vehicles over Unreliable Wireless Networks</h3>
    <p class="paper-authors">Sebastian Donnelly, Ruth Anderson, George Economides, James Broughton, Peter Ball, Alexander Rast, Andrew Bradley</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-16</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出一种基于计算机视觉的语义通信方法，通过将道路使用者分割信息编码为低分辨率灰度图像中的彩色高亮，在低带宽（&lt;500kbit/s）下实现低于200ms的端到端延迟，提升远程操作自动驾驶车辆的网络适应性。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.15258v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.15258v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Remote Operation is touted as being key to the rapid deployment of automated vehicles. Streaming imagery to control connected vehicles remotely currently requires a reliable, high throughput network connection, which can be limited in real-world remote operation deployments relying on public network infrastructure. This paper investigates how the application of computer vision assisted semantic communication can be used to circumvent data loss and corruption associated with traditional image compression techniques. By encoding the segmentations of detected road users into colour coded highlights within low resolution greyscale imagery, the required data rate can be reduced by 50 \% compared with conventional techniques, while maintaining visual clarity. This enables a median glass-to-glass latency of below 200ms even when the network data rate is below 500kbit/s, while clearly outlining salient road users to enhance situational awareness of the remote operator. The approach is demonstrated in an area of variable 4G mobile connectivity using an automated last-mile delivery vehicle. With this technique, the results indicate that large-scale deployment of remotely operated automated vehicles could be possible even on the often constrained public 4G/5G mobile network, providing the potential to expedite the nationwide roll-out of automated vehicles.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>远程操作自动驾驶车辆依赖高带宽、低延迟的网络，但公共移动网络（如4G/5G）在实际部署中常存在带宽限制和数据丢包问题，传统图像压缩技术难以在低带宽下保持关键信息的完整性与实时性。</p></div>
    <div class="detail-block"><h4>Method</h4><p>采用计算机视觉辅助的语义通信技术：先检测并分割道路使用者（如车辆、行人），将其轮廓编码为彩色高亮信息，叠加在低分辨率灰度图像上传输，从而减少数据传输量，避免传统压缩导致的关键信息损失。</p></div>
    <div class="detail-block"><h4>Result</h4><p>该方法将所需数据率较传统技术降低50%，在低于500kbit/s的网络带宽下仍能实现中值端到端延迟低于200ms，并清晰突出关键道路使用者，提升了远程操作员的情境感知能力。在4G网络环境下的自动最后一公里配送车辆实验中验证了有效性。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>该语义通信方法能有效克服公共移动网络的带宽限制，使远程操作自动驾驶车辆的大规模部署成为可能，有望加速自动驾驶车辆在全国范围的推广应用。</p></div>
  </details>
</article>
  </div>
</section>

<section id="query-lic" class="query-section" data-query-label="LIC 端到端" data-paper-count="3">
  <header class="query-header">
    <h2>LIC 端到端</h2>
    <p class="query-count">3 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context</h3>
    <p class="paper-authors">Tiange Zhang, Zhimeng Huang, Xiandong Meng, Kai Zhang, Zhipin Deng, Siwei Ma</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-14</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>提出L-STEC方法，通过LSTM扩展参考链以捕获长期依赖，并融合像素域空间上下文，提升神经视频压缩性能，显著节省码率。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.12790v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.12790v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame&#x27;s features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有神经视频压缩方法仅依赖前一帧特征预测时序上下文，导致两个问题：短参考窗口无法捕获长期依赖和精细纹理细节；仅传播特征级信息会累积误差，造成预测不准确和纹理丢失。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出长期时空增强上下文（L-STEC）方法：使用LSTM扩展参考链以捕获长期依赖；引入像素域的扭曲空间上下文，通过多感受野网络融合时空信息，以更好地保留参考细节。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，L-STEC通过丰富上下文信息显著提升压缩性能，与DCVC-TCM相比，在PSNR和MS-SSIM上分别节省37.01%和31.65%的码率，性能优于VTM-17.0和DCVC-FM，达到新的最优水平。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>L-STEC方法通过增强长期时空上下文，有效解决了现有神经视频压缩中的参考限制和误差累积问题，实现了显著的码率节省和性能提升，确立了新的技术标杆。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">Error-Propagation-Free Learned Video Compression With Dual-Domain Progressive Temporal Alignment</h3>
    <p class="paper-authors">Han Li, Shaohui Li, Wenrui Dai, Chenglin Li, Xinlong Pan, Haipeng Wang, Junni Zou, Hongkai Xiong</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-11</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出一种新的统一变换学习视频压缩框架，通过双域渐进时域对齐和基于质量的专家混合模块，解决了现有方法在运动估计/补偿不准确与误差传播之间的两难问题，实现了无误差传播且质量一致的流式压缩。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.10450v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.10450v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Existing frameworks for learned video compression suffer from a dilemma between inaccurate temporal alignment and error propagation for motion estimation and compensation (ME/MC). The separate-transform framework employs distinct transforms for intra-frame and inter-frame compression to yield impressive rate-distortion (R-D) performance but causes evident error propagation, while the unified-transform framework eliminates error propagation via shared transforms but is inferior in ME/MC in shared latent domains. To address this limitation, in this paper, we propose a novel unifiedtransform framework with dual-domain progressive temporal alignment and quality-conditioned mixture-of-expert (QCMoE) to enable quality-consistent and error-propagation-free streaming for learned video compression. Specifically, we propose dualdomain progressive temporal alignment for ME/MC that leverages coarse pixel-domain alignment and refined latent-domain alignment to significantly enhance temporal context modeling in a coarse-to-fine fashion. The coarse pixel-domain alignment efficiently handles simple motion patterns with optical flow estimated from a single reference frame, while the refined latent-domain alignment develops a Flow-Guided Deformable Transformer (FGDT) over latents from multiple reference frames to achieve long-term motion refinement (LTMR) for complex motion patterns. Furthermore, we design a QCMoE module for continuous bit-rate adaptation that dynamically assigns different experts to adjust quantization steps per pixel based on target quality and content rather than relies on a single quantization step. QCMoE allows continuous and consistent rate control with appealing R-D performance. Experimental results show that the proposed method achieves competitive R-D performance compared with the state-of-the-arts, while successfully eliminating error propagation.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有学习视频压缩框架存在两难：分离变换框架虽率失真性能好但误差传播明显；统一变换框架虽消除误差传播但运动估计/补偿在共享潜在域中效果较差。需克服此局限以实现高质量、无误差传播的压缩。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出双域渐进时域对齐：先用单参考帧光流进行粗略像素域对齐处理简单运动，再用多参考帧潜在域的流引导可变形Transformer进行精细潜在域对齐以处理复杂运动。同时设计基于质量的专家混合模块，根据目标质量和内容动态分配专家调整量化步长，实现连续码率适配。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，该方法在率失真性能上与最先进方法竞争，同时成功消除了误差传播，实现了连续一致的质量控制。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>所提框架通过双域渐进对齐和专家混合量化，有效平衡了运动估计精度与误差传播问题，为学习视频压缩提供了高质量、无误差传播的解决方案。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">Generative Neural Video Compression via Video Diffusion Prior</h3>
    <p class="paper-authors">Qi Mao, Hao Cheng, Tinghan Yang, Libiao Jin, Siwei Ma</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2025-12-04</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了GNVC-VD，首个基于DiT的生成式神经视频压缩框架，通过统一的流匹配潜在细化模块和视频扩散Transformer，在极低码率下提升感知质量并减少闪烁伪影。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2512.05016v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2512.05016v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有感知编解码器主要依赖预训练的图像生成先验来恢复高频细节，但其逐帧处理缺乏时间建模，导致感知闪烁伪影。需要一种能统一时空潜在压缩与序列级生成细化的视频原生生成先验方法。</p></div>
    <div class="detail-block"><h4>Method</h4><p>GNVC-VD构建于视频生成基础模型之上，引入统一的流匹配潜在细化模块，利用视频扩散Transformer通过序列级去噪联合增强帧内和帧间潜在表示；从解码的时空潜在初始化细化，学习适应压缩退化的校正项；通过条件适配器向DiT层注入压缩感知线索，以去除伪影并保持时间一致性。</p></div>
    <div class="detail-block"><h4>Result</h4><p>大量实验表明，GNVC-VD在感知质量上超越传统和学习的编解码器，显著减少了先前生成方法中持续的闪烁伪影，即使在低于0.01 bpp的极低码率下也有效。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>将视频原生生成先验整合到神经编解码器中，为下一代感知视频压缩带来了前景，GNVC-VD框架在保持时间一致性的同时提升了压缩效率。</p></div>
  </details>
</article>
  </div>
</section>

<section id="query-vsr" class="query-section" data-query-label="VSR 视频超分辨率" data-paper-count="3">
  <header class="query-header">
    <h2>VSR 视频超分辨率</h2>
    <p class="query-count">3 篇</p>
  </header>
  <div class="query-content">
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 1</p>
    <h3 class="paper-title">Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing</h3>
    <p class="paper-authors">Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-02-18</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出了一种名为视觉自我精炼（VSR）的新范式，通过让模型生成并可视化像素级定位输出，然后利用这些可视化反馈自我检查和纠正视觉感知错误，以提升大视觉语言模型在图表解析等复杂视觉任务上的准确性。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2602.16455v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2602.16455v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor&#x27;&#x27; to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points&#x27; Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有的大视觉语言模型在文本层面的推理和自我纠正能力较强，但在视觉密集的图表解析等任务中，常出现数据遗漏、错位和幻觉等错误，缺乏有效的视觉感知纠错机制。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出了VSR范式，并在图表解析领域实例化为ChartVSR模型。该方法将解析过程分为两个阶段：精炼阶段，通过迭代生成像素级定位的可视化反馈来确保数据点定位的准确性；解码阶段，利用已验证的定位作为精确的视觉锚点来解析最终的结构化数据。同时构建了新的高难度基准测试ChartP-Bench。</p></div>
    <div class="detail-block"><h4>Result</h4><p>VSR作为一种通用的视觉反馈机制，为提升视觉中心任务的准确性提供了新的方向；ChartVSR通过视觉自我精炼有效改善了图表解析的精度。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>VSR范式通过引入像素级定位的可视化反馈，使模型能够直观地检查和纠正自身的视觉感知错误，显著提升了在图表解析等复杂视觉任务上的性能，并具有扩展到更广泛视觉任务的潜力。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 2</p>
    <h3 class="paper-title">All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations</h3>
    <p class="paper-authors">Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-01-28</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文针对视频修复中未知退化类型和强度随时间平滑演变的挑战，提出了SEUD场景和ORCANet网络，通过物理先验和自适应提示实现高质量、时序一致的视频恢复。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2601.00533v2" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2601.00533v2" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>现有的一体化图像修复方法扩展到视频时，通常仅关注逐帧的退化变化，忽略了真实世界中退化过程（如类型和强度）的时序连续性。实际中，退化可能平滑演变、共存或逐渐过渡，现有方法对此建模不足。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出SEUD场景以描述平滑演变的未知退化；设计了一个时序连贯的视频退化合成流程；提出了ORCANet网络，包含基于物理先验的粗强度估计去雾模块（CIED）和生成静态/动态提示的流提示生成模块（FPG），并采用标签感知监督增强提示区分性。</p></div>
    <div class="detail-block"><h4>Result</h4><p>大量实验表明，ORCANet在恢复质量、时序一致性和鲁棒性上优于图像和视频基线方法。代码已开源。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>本文提出的SEUD场景和ORCANet网络有效解决了视频中退化平滑演变的修复问题，通过结合物理先验和自适应提示机制，实现了更符合真实世界需求的视频恢复。</p></div>
  </details>
</article>
<article class="paper-card" data-paper-card>
  <header class="paper-head">
    <p class="paper-index">No. 3</p>
    <h3 class="paper-title">OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion</h3>
    <p class="paper-authors">Shuoyan Wei, Feng Li, Chen Zhou, Runmin Cong, Yao Zhao, Huihui Bai</p>
    <div class="paper-meta-inline">
      <p class="meta-item"><span>来源</span>arxiv</p>
      <p class="meta-item"><span>更新时间</span>2026-01-28</p>
      <p class="meta-item taxonomy-item"><span>分类</span><strong class="primary-tag">cs.CV</strong> · cs.GR</p>
    </div>
    <p class="paper-tldr"><span>TLDR</span>本文提出OSDEnhancer，首个通过高效一步扩散过程实现真实世界时空视频超分辨率的框架，结合线性预插值、时序-空间专家混合与双向可变形VAE解码器，在复杂退化下取得SOTA性能。</p>
  </header>
  <div class="paper-links has-links">
    <a class="paper-link" href="https://arxiv.org/pdf/2601.20308v1" target="_blank" rel="noopener noreferrer">PDF</a>
    <a class="paper-link" href="https://arxiv.org/abs/2601.20308v1" target="_blank" rel="noopener noreferrer">Abstract</a>
  </div>
  <details class="paper-details" data-paper-details>
    <summary class="details-summary">
      <span>阅读摘要与笔记</span>
      <span class="summary-hint">点击展开</span>
    </summary>
    <div class="detail-block">
      <h4>Abstract</h4>
      <p>Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.</p>
    </div>
    <div class="detail-block"><h4>Motivation</h4><p>扩散模型在视频超分辨率中表现出色，但其在时空视频超分辨率（需同时提升分辨率与帧率并保持时序一致性）的潜力尚未充分探索；现有方法多基于简化退化假设，难以应对真实世界中复杂未知退化，重建保真度与时序一致性的高要求使鲁棒框架开发极具挑战。</p></div>
    <div class="detail-block"><h4>Method</h4><p>提出OSDEnhancer框架：1）使用线性预插值初始化关键时空结构；2）训练时序细化与空间增强专家混合模型，使不同专家路径分别学习时序一致性与空间细节的鲁棒专长表示，并在推理中协同增强；3）引入双向可变形变分自编码器解码器，通过循环时空聚合与传播提升跨帧重建保真度。</p></div>
    <div class="detail-block"><h4>Result</h4><p>实验表明，该方法在真实世界场景中实现了最先进的性能，同时保持了优异的泛化能力。</p></div>
    <div class="detail-block"><h4>Conclusion</h4><p>OSDEnhancer通过一步扩散过程有效解决了真实世界时空视频超分辨率的挑战，在复杂退化下实现了高保真重建与时序一致性，为相关领域提供了新颖且鲁棒的解决方案。</p></div>
  </details>
</article>
  </div>
</section>
    </main>
  </div>

  <script src="./assets/interactive.js"></script>
</body>
</html>
