
# WARNING:
# Do not edit this default config directly.
# Copy it to cofig/custom.yml and edit your personal settings there.
# Run with: paper-tracker --config config/custom.yml search


# runtime domain
log:
  level: INFO
  to_file: true
  dir: log

# storage domain
storage:
  enabled: true
  db_path: database/papers.db  # Relative path (from working directory) or absolute path (starts with /)
  content_storage_enabled: true  # save full paper content (default: true)
  keep_arxiv_version: false  # false: 2601.21922, true: 2601.21922v1

# search domain - Optional global scope applied to every query.
# Field keys must be uppercase: TITLE/ABSTRACT/AUTHOR/JOURNAL/CATEGORY
# Operator keys must be uppercase: AND/OR/NOT
#
# scope:
#   CATEGORY:
#     OR: [cs.CV]

queries:
  - NAME: example
    OR:
      - machine learning

search:
  max_results: 5              # max paper count for each query

  # multi round fetch
  pull_every: 7               # Strict time window, unit: day
  fill_enabled: true          # Allow fill with paper outside the strict time window
  max_lookback_days: 30       # Max length for fill window, -1 as infinite
  max_fetch_items: 125        # Max count for item tried to fetch at a time, -1 as infinite
  fetch_batch_size: 25        # Fetch count for each round

# output domain
output:
  base_dir: output/              # Output root directory
  formats: [console, json]             # Output formats: console, json, markdown, html (can select multiple)

  # Markdown export configuration
  markdown:
    template_dir: template/markdown/
    document_template: document.md       # Document-level template
    paper_template: paper.md             # Paper-level template
    paper_separator: "\n\n---\n\n"       # Separator between papers

  # HTML export configuration
  html:
    template_dir: template/html/scholar/
    document_template: document.html
    paper_template: paper.html

# llm domain
llm:
  enabled: false                          # Enable/disable LLM translation features
  provider: openai-compat                 # Provider type (currently only this)
  api_key_env: LLM_API_KEY                # Environment variable for API key
  timeout: 30                             # Request timeout (seconds)
  target_lang: zh                         # Target language (zh/en/ja/ko/fr/de/es)
  temperature: 0.0                        # Sampling temperature (0.0 = deterministic)
  max_tokens: 800                         # Maximum response tokens
  max_workers: 3                          # Parallel translation workers

  # Feature selection
  # You can choose one of three modes:
  # 1. Translation only: enable_translation=true, enable_summary=false
  # 2. Summary only: enable_translation=false, enable_summary=true
  # 3. Both: enable_translation=true, enable_summary=true
  enable_translation: true                # Enable abstract translation
  enable_summary: false                   # Enable paper summary (TLDR, motivation, method, result, conclusion)
                                          # Default: false (to control API costs)

  # Retry configuration (for handling timeouts and transient errors)
  max_retries: 3                          # Maximum retry attempts (0 to disable)
  retry_base_delay: 1.0                   # Base delay for exponential backoff (seconds)
  retry_max_delay: 10.0                   # Maximum delay between retries (seconds)
  retry_timeout_multiplier: 1.0           # Timeout multiplier per retry (1.0 = no change)

  base_url: https://api.openai.com        # API base URL
  model: gpt-4o-mini                      # Model identifier
# Provider-specific configuration examples:
#
# OpenAI GPT-4o-mini (recommended for cost):
# llm:
#   enabled: true
#   base_url: https://api.openai.com
#   model: gpt-4o-mini
#
# DeepSeek (very cost-effective):
# llm:
#   enabled: true
#   base_url: https://api.deepseek.com
#   model: deepseek-chat
#
# SiliconFlow (free tier available):
# llm:
#   enabled: true
#   base_url: https://api.siliconflow.cn
#   model: Qwen/Qwen2.5-7B-Instruct
