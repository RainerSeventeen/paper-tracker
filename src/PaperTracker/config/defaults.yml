# Internal defaults.
#
# DO NOT edit this file to configure your instance.
# To override any value, add the corresponding key to your own config file
# (passed via --config). It will be deep-merged on top of these defaults.
#
# Required fields with no defaults — must be specified in user config:
#   queries          — define at least one search query
#   llm.base_url     — required when llm.enabled is true
#   llm.model        — required when llm.enabled is true

# runtime domain
log:
  level: INFO
  to_file: true
  dir: log

# storage domain
storage:
  enabled: true
  db_path: database/papers.db     # Relative path (from working directory) or absolute path (starts with /)
  content_storage_enabled: true   # save full paper content (default: true)
  keep_arxiv_version: false       # false: 2601.21922, true: 2601.21922v1

# Global scope (optional), applied to every query.
# Field keys must be uppercase: TITLE/ABSTRACT/AUTHOR/JOURNAL/CATEGORY
# Operator keys must be uppercase: AND/OR/NOT
#
# scope:
#   CATEGORY:
#     OR: [cs.CV]

search:
  sources: [arxiv]           # enabled search sources (arxiv, crossref)
  max_results: 5              # max paper count for each query

  # multi round fetch
  pull_every: 7               # Strict time window, unit: day
  fill_enabled: true          # Allow fill with paper outside the strict time window
  max_lookback_days: 30       # Max length for fill window, -1 as infinite
  max_fetch_items: 125        # Max number of items to fetch, -1 means unlimited
  fetch_batch_size: 25        # Fetch count for each round

# output domain
output:
  base_dir: output/                      # Output root directory
  formats: [console, json, html]         # Output formats: console, json, markdown, html (can select multiple)

  # Markdown export configuration
  markdown:
    template_dir: template/markdown/
    document_template: document.md       # Document-level template
    paper_template: paper.md             # Paper-level template
    paper_separator: "\n\n---\n\n"       # Separator between papers

  # HTML export configuration
  html:
    template_dir: template/html/scholar/
    document_template: document.html
    paper_template: paper.html

# llm domain
llm:
  enabled: false                          # Enable/disable LLM features
  provider: openai-compat                 # Provider type (currently only this)
  api_key_env: LLM_API_KEY                # Environment variable for API key
  timeout: 30                             # Request timeout (seconds)
  target_lang: Simplified Chinese         # Target language instruction passed to LLM
  temperature: 0.0                        # Sampling temperature (0.0 = deterministic)
  max_tokens: 800                         # Maximum response tokens
  max_workers: 3                          # Parallel translation workers

  # Feature selection
  # You can choose one of three modes:
  # 1. Translation only: enable_translation=true, enable_summary=false
  # 2. Summary only: enable_translation=false, enable_summary=true
  # 3. Both: enable_translation=true, enable_summary=true
  enable_translation: false                # Enable abstract translation
  enable_summary: false                   # Enable paper summary (TLDR, motivation, method, result, conclusion)
                                          # Default: false (to control API costs)

  # Retry configuration (for handling timeouts and transient errors)
  max_retries: 3                          # Maximum retry attempts (0 to disable)
  retry_base_delay: 1.0                   # Base delay for exponential backoff (seconds)
  retry_max_delay: 10.0                   # Maximum delay between retries (seconds)
  retry_timeout_multiplier: 1.0           # Timeout multiplier per retry (1.0 = no change)

  # base_url and model are required when llm.enabled is true.
  # Provider-specific configuration examples:
  #
  # OpenAI GPT-4o-mini (recommended for cost):
  #   base_url: https://api.openai.com
  #   model: gpt-4o-mini
  #
  # DeepSeek (very cost-effective):
  #   base_url: https://api.deepseek.com
  #   model: deepseek-chat
  #
  # SiliconFlow (free tier available):
  #   base_url: https://api.siliconflow.cn
  #   model: Qwen/Qwen2.5-7B-Instruct
